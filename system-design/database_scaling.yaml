- title: "Database Scaling - Horizontal vs Vertical"
  difficulty: "easy"
  tags: ["database", "scaling", "horizontal", "vertical"]
  Front: |
    What is the difference between **horizontal** and **vertical** scaling for databases?
  Back: |
    **Vertical scaling (scale up):** Bigger machine — more CPU, RAM, faster SSD.
    - Simple — no application changes required
    - Has a **ceiling** — hardware has physical limits
    - Single point of failure

    **Horizontal scaling (scale out):** More machines — sharding, replication.
    - Near-infinite scale
    - Requires **application-level changes** (sharding logic, distributed queries)
    - Complex operational overhead

    **In practice:** Most systems start vertical (it is cheaper and simpler) and move to horizontal scaling when they hit hardware limits or need fault tolerance.

    **Interview tip:** Mention both approaches and explain why you would start with vertical. Interviewers want to see that you understand the trade-off, not just that you know the buzzwords.

- title: "Database Scaling - Leader-Follower Replication"
  difficulty: "medium"
  tags: ["database", "replication", "leader-follower", "consistency"]
  Front: |
    What is **leader-follower replication** and what problem does it solve?
  Back: |
    One **leader** (primary) handles all writes. Multiple **followers** (replicas) replicate the leader's data and serve read queries.

    **What it solves:** Read scalability. Most applications are read-heavy (90%+ reads), so distributing reads across followers dramatically reduces load on the leader.

    **Trade-offs:**
    - **Replication lag:** Followers may serve slightly stale data (eventual consistency)
    - **Failover risk:** If the leader dies, a follower must be promoted. Un-replicated writes may be lost during the switch.
    - **Write bottleneck:** All writes still go through a single leader

    **Interview tip:** Always mention replication lag when discussing leader-follower setups. The follow-up question is usually about what happens during failover — explain the split-brain risk and how consensus protocols help.

- title: "Database Scaling - Sharding"
  difficulty: "hard"
  tags: ["database", "sharding", "partitioning", "distributed"]
  Front: |
    What is **database sharding** and what are the common sharding strategies?
  Back: |
    Sharding splits data across multiple independent databases so no single machine stores everything.

    **Sharding strategies:**
    - **Range-based:** e.g., users A-M on shard 1, N-Z on shard 2. Simple but prone to **hotspots** if data is unevenly distributed.
    - **Hash-based:** `hash(user_id) % N` determines the shard. Even distribution, but adding a shard remaps almost all keys.
    - **Directory-based:** A lookup table maps each key to its shard. Flexible but the directory is a single point of failure.

    **Challenges:**
    - **Cross-shard queries** are expensive (joins across databases)
    - **Rebalancing** when adding shards requires data migration
    - **Referential integrity** across shards is hard to enforce

    **Consistent hashing** minimizes data movement when shards change. In interviews, mentioning it shows depth.

- title: "Database Scaling - Read Replicas"
  difficulty: "easy"
  tags: ["database", "read replica", "scaling", "reads"]
  Front: |
    What is a **read replica** and when would you use one?
  Back: |
    A read replica is a **copy of the primary database** that serves read-only queries.

    **When to use:**
    - Reads vastly outnumber writes (typical web apps are 90%+ reads)
    - You need to reduce load on the primary database
    - Reporting or analytics queries that would slow down production traffic

    **How it works:** The application routes reads to replicas and writes to the primary. A replication mechanism keeps replicas in sync.

    **Trade-off:** Replication lag means reads may be **slightly stale**. For most applications this is acceptable, but not for use cases requiring strong consistency (e.g., checking account balance right after a transfer).

    **Not useful when:** Your bottleneck is write throughput — read replicas do not help with write-heavy workloads.

- title: "Database Scaling - Connection Pooling"
  difficulty: "medium"
  tags: ["database", "connection pooling", "performance"]
  Front: |
    What is **database connection pooling** and why does it matter at scale?
  Back: |
    A connection pool maintains a set of **reusable database connections** shared across application requests.

    **Why it matters:** Creating a new DB connection is expensive:
    - TCP handshake
    - Authentication
    - SSL negotiation
    - Server-side memory allocation

    Without pooling, each request opens and closes a connection. At 1,000 requests per second, that means 1,000 connection setups and teardowns. Most databases have a **maximum connection limit** (e.g., PostgreSQL defaults to 100).

    **How it works:** The pool keeps ~50-100 connections open. Requests **borrow** a connection, use it, and **return** it to the pool. New requests reuse existing connections instantly.

    **Common tools:** PgBouncer (PostgreSQL), HikariCP (Java), SQLAlchemy pool (Python).

    **Interview tip:** Connection pooling is often overlooked in system design answers. Mentioning it shows you understand real-world database operations beyond just schema design.
