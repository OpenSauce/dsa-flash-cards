- title: "Consistent Hashing - Modulo Problem"
  difficulty: "medium"
  tags: ["consistent hashing", "modulo", "distributed systems"]
  Front: |
    Why does naive **modulo hashing** (`key % N`) fail for distributed systems?
  Back: |
    When you add or remove a server, **N changes**, and almost every key maps to a different server.

    With N servers, adding one server remaps approximately **(N-1)/N** of all keys. For example, going from 10 to 11 servers remaps ~91% of keys.

    **At scale, this is catastrophic:**
    - Millions of cached keys become invalid simultaneously
    - All requests hit the backing store at once (**cache stampede**)
    - The system may collapse under the sudden load

    **The core problem:** Modulo hashing creates a **global dependency** — every key's assignment depends on the total server count, so changing any server affects nearly all keys.

    **Interview tip:** This is the motivation for consistent hashing. Start by explaining why naive hashing fails before describing the solution.

- title: "Consistent Hashing - How It Works"
  difficulty: "medium"
  tags: ["consistent hashing", "hash ring", "distributed systems"]
  Front: |
    How does **consistent hashing** minimize key redistribution when servers change?
  Back: |
    Map both servers and keys onto a **hash ring** (0 to 2^32). Each key is assigned to the **next server clockwise** on the ring.

    **When a server is added:** Only keys between the new server and its predecessor are remapped to the new server. All other keys stay on their current server.

    **When a server is removed:** Only its keys move to the next server clockwise. All other keys are unaffected.

    **Result:** On average, only **K/N keys** move when a server changes (K = total keys, N = servers). Compare this to modulo hashing where (N-1)/N keys move.

    **Key insight:** Each key's assignment depends only on its **nearest server on the ring**, not on the total server count. This is what makes it "consistent" — most mappings remain stable through changes.

- title: "Consistent Hashing - Virtual Nodes"
  difficulty: "hard"
  tags: ["consistent hashing", "virtual nodes", "load distribution"]
  Front: |
    What are **virtual nodes** in consistent hashing and why are they needed?
  Back: |
    With few physical servers on the hash ring, the distribution is **uneven** — some servers own larger arc segments and handle disproportionately more keys.

    **Virtual nodes solve this:** Each physical server gets **multiple positions** on the ring (e.g., 150 virtual nodes each). Instead of one point per server, there are many, spread across the ring.

    **Benefits:**
    - **Even load distribution:** Many small segments average out to proportional shares
    - **Smoother rebalancing:** Adding a server redistributes load across many existing servers, not just one neighbor
    - **Heterogeneous capacity:** More powerful servers can have more virtual nodes

    **Used by:** Cassandra, DynamoDB, and most production distributed caches.

    **Interview tip:** Always mention virtual nodes when discussing consistent hashing. Without them, the "even distribution" promise of consistent hashing does not hold in practice.

- title: "Consistent Hashing - Real-World Uses"
  difficulty: "medium"
  tags: ["consistent hashing", "applications", "distributed systems"]
  Front: |
    Name **3 systems** that use consistent hashing and explain what they use it for.
  Back: |
    1. **Cassandra:** Partitions data across nodes in the ring. Each node owns a range of the hash space, and the partition key determines which node stores each row.

    2. **DynamoDB:** Distributes key-value pairs across storage nodes. Uses consistent hashing with virtual nodes for automatic data partitioning and rebalancing.

    3. **Memcached / Redis clusters:** Routes cache keys to the correct shard. Client libraries use consistent hashing so adding a cache server only invalidates ~1/N of existing cached keys.

    **Also used by:**
    - **CDNs:** Map content to edge servers
    - **Load balancers:** IP-hash routing for session stickiness

    **Core value:** Any node can **independently compute** which server owns a key without coordination or a central directory. This makes the system decentralized and fast.
