# ── LRU CACHE ──────────────────────────────────────────────────────────────
- title: "LRU Cache - Overview"
  difficulty: "medium"
  tags: ["lru cache", "identify"]
  Front: |
    Which data structure …

    - Combines a **hash map + doubly linked list**
    - Keeps most-recently-used items at the front
    - Supports **get** and **put/evict** in **O(1)**?
  Back: |
    **LRU Cache**

    A fixed-capacity cache that evicts the least recently used entry when full, combining a hash map (O(1) lookup) with a doubly linked list (O(1) reordering). Use when you need bounded-memory caching with temporal locality -- web caches, database page buffers, API rate limiting. Trade-off: higher per-entry memory overhead than a simple hash map due to the linked list pointers and dual data structures.

- title: "LRU Cache – Get"
  difficulty: "medium"
  tags: ["lru cache", "get"]
  Front: |
    What operation does this implement, and what is its time complexity?

    ```go
    if node, ok := m[key]; ok {
        moveToFront(node)
        return node.val
    }
    return -1
    ```
  Back: |
    LRU get & promote to front (O(1))

    The hash map gives O(1) lookup; the doubly linked list gives O(1) move-to-front. Together they satisfy the LRU contract: the tail is always the least-recently-used item.

- title: "LRU Cache – Put"
  difficulty: "medium"
  tags: ["lru cache", "put"]
  Front: |
    What operation does this implement, and what is its time complexity?

    ```go
    if node, ok := m[key]; ok {
        node.val = val
        moveToFront(node)
    } else {
        if len(m) == cap { evictTail() }
        node := &Node{key: key, val: val}
        addFront(node)
        m[key] = node
    }
    ```
  Back: |
    LRU put with possible eviction (O(1))

    On capacity overflow, the tail node (least recently used) is evicted. Storing the key inside each node lets you delete the map entry during eviction without a reverse lookup.
